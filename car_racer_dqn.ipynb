{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from keras import losses\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, TimeDistributed, Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import RMSprop\n",
    "import tensorflow as tf\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize environment and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num actions: 4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Breakout-v0\")\n",
    "num_actions = env.action_space.n\n",
    "print(\"Num actions:\", num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (84, 84, 4)\n",
    "def init_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, input_shape=input_shape, kernel_size=(8,8),\n",
    "                                     activation=\"relu\", strides=4))\n",
    "    model.add(Conv2D(64, kernel_size=(8,8), activation=\"relu\", strides=2))\n",
    "    model.add(Conv2D(64, kernel_size=(3,3), activation=\"relu\"))\n",
    "    model.add(Dense(512, activation=\"relu\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(num_actions))\n",
    "    rms = RMSprop(lr=1e-4)\n",
    "    model.compile(optimizer=rms, loss=\"mse\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_model = init_model()\n",
    "target_q_model = init_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame, prev):\n",
    "    m = np.maximum(frame, prev)\n",
    "    m = rgb2gray(m)\n",
    "    return resize(m, (84, 84), anti_aliasing=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,3,4, 5])\n",
    "b = np.stack([a,a,a,a], axis=-1)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_epsilon(decay_period, steps_taken, min_epsilon):\n",
    "    step_size = (1 - min_epsilon) / decay_period\n",
    "    return max(min_epsilon, 1. - (step_size * steps_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miki/.pyenv/versions/3.6.8/lib/python3.6/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 0\n",
      "Adjusting target network\n",
      "epsilon: 0.9999991\n",
      "frames played: 1\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 1\n",
      "epsilon: 0.9999982\n",
      "frames played: 2\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 2\n",
      "epsilon: 0.9999973\n",
      "frames played: 3\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 3\n",
      "epsilon: 0.9999964\n",
      "frames played: 4\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 4\n",
      "epsilon: 0.9999955\n",
      "frames played: 5\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 5\n",
      "epsilon: 0.9999946\n",
      "frames played: 6\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 6\n",
      "epsilon: 0.9999937\n",
      "frames played: 7\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 7\n",
      "epsilon: 0.9999928\n",
      "frames played: 8\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 8\n",
      "epsilon: 0.9999919\n",
      "frames played: 9\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 9\n",
      "epsilon: 0.999991\n",
      "frames played: 10\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 10\n",
      "epsilon: 0.9999901\n",
      "frames played: 11\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 11\n",
      "epsilon: 0.9999892\n",
      "frames played: 12\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 12\n",
      "epsilon: 0.9999883\n",
      "frames played: 13\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 13\n",
      "epsilon: 0.9999874\n",
      "frames played: 14\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 14\n",
      "epsilon: 0.9999865\n",
      "frames played: 15\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 15\n",
      "epsilon: 0.9999856\n",
      "frames played: 16\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 16\n",
      "epsilon: 0.9999847\n",
      "frames played: 17\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 17\n",
      "epsilon: 0.9999838\n",
      "frames played: 18\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 18\n",
      "epsilon: 0.9999829\n",
      "frames played: 19\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 19\n",
      "epsilon: 0.999982\n",
      "frames played: 20\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 20\n",
      "epsilon: 0.9999811\n",
      "frames played: 21\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 21\n",
      "epsilon: 0.9999802\n",
      "frames played: 22\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 22\n",
      "epsilon: 0.9999793\n",
      "frames played: 23\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 23\n",
      "epsilon: 0.9999784\n",
      "frames played: 24\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 24\n",
      "epsilon: 0.9999775\n",
      "frames played: 25\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 25\n",
      "epsilon: 0.9999766\n",
      "frames played: 26\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 26\n",
      "epsilon: 0.9999757\n",
      "frames played: 27\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 27\n",
      "epsilon: 0.9999748\n",
      "frames played: 28\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 5}\n",
      "---------------------\n",
      "t: 28\n",
      "epsilon: 0.9999739\n",
      "frames played: 29\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 4}\n",
      "---------------------\n",
      "t: 29\n",
      "epsilon: 0.999973\n",
      "frames played: 30\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 4}\n",
      "---------------------\n",
      "t: 30\n",
      "epsilon: 0.9999721\n",
      "frames played: 31\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 4}\n",
      "---------------------\n",
      "t: 31\n",
      "epsilon: 0.9999712\n",
      "frames played: 32\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 4}\n",
      "---------------------\n",
      "t: 32\n",
      "epsilon: 0.9999703\n",
      "frames played: 33\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 4}\n",
      "---------------------\n",
      "t: 33\n",
      "epsilon: 0.9999694\n",
      "frames played: 34\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 4}\n",
      "---------------------\n",
      "t: 34\n",
      "epsilon: 0.9999685\n",
      "frames played: 35\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 4}\n",
      "---------------------\n",
      "t: 35\n",
      "epsilon: 0.9999676\n",
      "frames played: 36\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 4}\n",
      "---------------------\n",
      "t: 36\n",
      "epsilon: 0.9999667\n",
      "frames played: 37\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 4}\n",
      "---------------------\n",
      "t: 37\n",
      "epsilon: 0.9999658\n",
      "frames played: 38\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 4}\n",
      "---------------------\n",
      "t: 38\n",
      "epsilon: 0.9999649\n",
      "frames played: 39\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 4}\n",
      "---------------------\n",
      "t: 39\n",
      "epsilon: 0.999964\n",
      "frames played: 40\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 4}\n",
      "---------------------\n",
      "t: 40\n",
      "epsilon: 0.9999631\n",
      "frames played: 41\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 4}\n",
      "---------------------\n",
      "t: 41\n",
      "epsilon: 0.9999622\n",
      "frames played: 42\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 4}\n",
      "---------------------\n",
      "t: 42\n",
      "epsilon: 0.9999613\n",
      "frames played: 43\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 4}\n",
      "---------------------\n",
      "t: 43\n",
      "epsilon: 0.9999604\n",
      "frames played: 44\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 4}\n",
      "---------------------\n",
      "t: 44\n",
      "epsilon: 0.9999595\n",
      "frames played: 45\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 4}\n",
      "---------------------\n",
      "t: 45\n",
      "epsilon: 0.9999586\n",
      "frames played: 46\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 4}\n",
      "---------------------\n",
      "t: 46\n",
      "epsilon: 0.9999577\n",
      "frames played: 47\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 4}\n",
      "---------------------\n",
      "t: 47\n",
      "epsilon: 0.9999568\n",
      "frames played: 48\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 4}\n",
      "---------------------\n",
      "t: 48\n",
      "epsilon: 0.9999559\n",
      "frames played: 49\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 4}\n",
      "---------------------\n",
      "t: 49\n",
      "epsilon: 0.999955\n",
      "frames played: 50\n",
      "reward: 0.0\n",
      "terminal: False\n",
      "info: {'ale.lives': 4}\n",
      "---------------------\n",
      "t: 50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6ecc0cedb5cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mGAMMA\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_s_next\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_phi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.8/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPISODES = int(4e5)\n",
    "TIME_STEPS = int(1e6)\n",
    "memories = deque(maxlen=int(1e6))\n",
    "NUM_ACTIONS = env.action_space.n\n",
    "GAMMA = 0.99\n",
    "EPSILON_DECAY_PERIOD = 2e5\n",
    "MIN_EPSILON = 0.1\n",
    "C = 1000\n",
    "BATCH_SIZE = 32\n",
    "SEQ_LEN = 4\n",
    "f_recent = deque(maxlen=SEQ_LEN)\n",
    "losses = []\n",
    "MAX_FRAMES = 10e6\n",
    "frames_played = 0\n",
    "\n",
    "for i_episode in range(NUM_EPISODES):\n",
    "    if frames_played >= MAX_FRAMES:\n",
    "        target_q_model.set_weights(q_model.get_weights())\n",
    "        target_q_model.save('car_dqn_model.h5')\n",
    "        break\n",
    "    env.reset()\n",
    "    action = env.action_space.sample()\n",
    "    prev, _, _, _ = env.step(action)\n",
    "    for i in range(SEQ_LEN):\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, terminal, info = env.step(action)\n",
    "        s_t = preprocess_frame(observation, prev)\n",
    "        f_recent.append(s_t)\n",
    "        prev = observation\n",
    "    phi_t = np.stack(np.array(f_recent), axis=-1)\n",
    "    loss = 0\n",
    "    for t in range(TIME_STEPS):\n",
    "        frames_played += 1\n",
    "        epsilon = calculate_epsilon(EPSILON_DECAY_PERIOD, frames_played, MIN_EPSILON)\n",
    "        print(\"t:\", t)\n",
    "        # With some probability epsilon select random a. \n",
    "        if np.random.uniform() <= epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_model.predict(np.expand_dims(np.array(phi_t), axis=0)))\n",
    "        observation, reward, terminal, info = env.step(action)\n",
    "        s_t_next = preprocess_frame(observation, prev)\n",
    "        prev = observation\n",
    "        f_recent.append(s_t_next)\n",
    "        phi_t_next = np.stack(np.array(f_recent), axis=-1)\n",
    "        mem = (phi_t, action, reward, phi_t_next, terminal)\n",
    "        memories.append(mem)\n",
    "        s_t = s_t_next\n",
    "        phi_t = phi_t_next\n",
    "        mem_batch = random.sample(memories, min(BATCH_SIZE, len(memories)))\n",
    "        m_phi, m_action, m_reward, m_phi_next, m_terminal = zip(*mem_batch)\n",
    "        y = q_model.predict(np.array(m_phi))\n",
    "        q_s_next = target_q_model.predict(np.array(m_phi_next))\n",
    "        for i, (mt, mr, ma) in enumerate(zip(m_terminal, m_reward, m_action)):\n",
    "            if mt:\n",
    "                y[i][ma] = mr\n",
    "            else:\n",
    "                y[i][ma] = mr + GAMMA * np.max(q_s_next[i])\n",
    "\n",
    "        l = q_model.train_on_batch(np.array(m_phi), y)\n",
    "        loss += l\n",
    "        if t % C == 0:\n",
    "            print(\"Adjusting target network\")\n",
    "            target_q_model.set_weights(q_model.get_weights())\n",
    "            target_q_model.save('car_dqn_model.h5')\n",
    "        print(\"epsilon: {}\".format(epsilon))\n",
    "        print(\"frames played: {}\".format(frames_played))\n",
    "        print(\"reward: {}\".format(reward))\n",
    "        print(\"terminal: {}\".format(terminal))\n",
    "        print(\"info: {}\".format(info))\n",
    "        print(\"---------------------\")\n",
    "        if terminal:\n",
    "            losses.append(loss)\n",
    "            print(\"Loss after {}th episode: {}\".format(i_episode, loss))\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            target_q_model.set_weights(q_model.get_weights())\n",
    "            target_q_model.save('car_dqn_model.h5')\n",
    "            break\n",
    "env.close()\n",
    "\n",
    "plt.plot(list(range(len(losses))), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = deque(maxlen=3)\n",
    "a.append((1,1))\n",
    "a.append((2,2))\n",
    "a.append((3,3))\n",
    "b,c = zip(*a)\n",
    "for i, (x, y) in enumerate(zip(b,c)):\n",
    "    print(i, x, y)\n",
    "print(b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
